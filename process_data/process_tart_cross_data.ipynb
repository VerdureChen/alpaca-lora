{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# process tart data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "tart_path = '/home1/cxy/tart/ranker_train_full_contriever_minilm_denoised_w_instruction_unfollowing_10k_added_oqa_fixed/st_train_ranker_input.json'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/cxy/anaconda3/envs/alpaca-lora/lib/python3.10/site-packages/datasets/load.py:1748: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=no_checks' instead.\n",
      "  warnings.warn(\n",
      "Found cached dataset json (/home1/cxy/.cache/huggingface/datasets/json/default-ff83d81b6f54fc0a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ca4a6f0c60b44568c9ebb0fcac16ec1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "tart_data = datasets.load_dataset('json', data_files=tart_path, ignore_verifications=True)['train']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define tokenizer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import random\n",
    "random.seed(42)\n",
    "tokenizer = LlamaTokenizer.from_pretrained('/home1/cxy/alpaca-lora/llama-7b-hf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# count max document word length\n",
    "from tqdm import tqdm\n",
    "def count_max_doc_token_length(data):\n",
    "    max_doc_token_length = 0\n",
    "    for i in tqdm(range(len(data))):\n",
    "        # doc_token_length = len(tokenizer(data[i]['document'], return_tensors='pt')['input_ids'][0])\n",
    "        doc_token_length = len(data[i]['document'].split(' '))\n",
    "        if doc_token_length > max_doc_token_length:\n",
    "            max_doc_token_length = doc_token_length\n",
    "    return max_doc_token_length\n",
    "\n",
    "# max_doc_token_length = count_max_doc_token_length(tart_data)\n",
    "# print(max_doc_token_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'Find an evidence paragraph on the web with evidence and answer for this </s> what causes fistulas',\n 'document': \"web paragraph Dan Johnson. Echo Park is located in the heart of Dinosaur's canyon country. Here, the Yampa River flows into the Green River, which winds around the massive feature known as Steamboat Rock. The meeting of the two rivers along with nearby geologic faults created some of the monument's most dramatic scenery.\",\n 'label': 0}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tart_data[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# for each query, add an id, for each document, add another id\n",
    "def add_ids(data, idx):\n",
    "    data['id'] = idx\n",
    "    return data\n",
    "# tart_data = tart_data.map(add_ids, with_indices=True)\n",
    "# tart_data[255]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# chunk the document into 256 word chunks, and add the question to each chunk, then evaluate the model on each chunk and take the chunk with the highest score as the representation of the document for the question\n",
    "\n",
    "def chunk_examples(examples):\n",
    "    chunks = []\n",
    "    ids = []\n",
    "    queries = []\n",
    "    labels = []\n",
    "    for sentence, label, query, tid in zip(examples[\"document\"], examples[\"label\"], examples[\"query\"], examples[\"id\"]):\n",
    "        chunks += [' '.join(sentence.split(' ')[i:i + 256]) for i in range(0, len(sentence.split(' ')), 128)]\n",
    "        for i in range(0, len(sentence.split(' ')), 128):\n",
    "            ids.append(tid)\n",
    "            queries.append(query)\n",
    "            labels.append(label)\n",
    "    return {\"chunks\": chunks, \"ids\":ids, \"queries\":queries, \"labels\":labels}\n",
    "# chunked_dataset = tart_data.map(chunk_examples, batched=True, remove_columns=tart_data.column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "Saving the dataset (0/10 shards):   0%|          | 0/4712048 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd3d35d7d2244141bb6f80178ecb2d6f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunk_save_path ='/home1/cxy/alpaca-lora/process_data/chunked_tart_data'\n",
    "chunked_dataset.save_to_disk(chunk_save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home1/cxy/alpaca-lora/process_data\n",
      "Adding source directory to the sys.path: '/home1/cxy/alpaca-lora/tart'\n",
      "Using 8 GPUs\n"
     ]
    },
    {
     "data": {
      "text/plain": "DataParallel(\n  (module): EncT5ForSequenceClassification(\n    (shared): Embedding(32128, 1024)\n    (encoder): T5Stack(\n      (embed_tokens): Embedding(32128, 1024)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n                (relative_attention_bias): Embedding(32, 16)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n                (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n                (wo): Linear(in_features=2816, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-23): 23 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n                (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n                (wo): Linear(in_features=2816, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (classifier): Linear(in_features=1024, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model on each chunk and take the chunk with the highest score as the representation of the document for the question\n",
    "import sys\n",
    "import os.path as osp\n",
    "\n",
    "SRC_SUBDIR = '/home1/cxy/alpaca-lora/tart'\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "if SRC_SUBDIR not in sys.path:\n",
    "    print(f'Adding source directory to the sys.path: {SRC_SUBDIR!r}')\n",
    "    sys.path.insert(1, SRC_SUBDIR)\n",
    "from tart.modeling_enc_t5 import EncT5ForSequenceClassification\n",
    "from tart.tokenization_enc_t5 import EncT5Tokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "import numpy as np\n",
    "# load TART full and tokenizer\n",
    "model = EncT5ForSequenceClassification.from_pretrained(\"facebook/tart-full-flan-t5-xl\")\n",
    "tokenizer =  EncT5Tokenizer.from_pretrained(\"facebook/tart-full-flan-t5-xl\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = DataParallel(model)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import datasets\n",
    "chunked_dataset = datasets.load_from_disk('/home1/cxy/alpaca-lora/process_data/chunked_tart_data')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "{'chunks': \"search snippets Spin City (TV Series 19962002) - IMDb  Comedy ... With Michael J. Fox, Charlie Sheen, Heather Locklear, Richard Kind. ... about the Winston administration's links to organized crime, Mike decides to take ..... In this sitcom, Deputy Mayor Mike Flaherty (Fox) and his City Hall staff must stop ... (Gaberman/Chaplin)the Mayor's Speech Writer, and Nikki Faber (Britton)...\",\n 'ids': 1509,\n 'queries': 'Find the top 5 Web snippets that answer this </s> Mayor Randall Winston,Deputy Mayor Michael Flaherty,Nikki Faber',\n 'labels': 1}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_dataset[2333]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# replace </s> with <SEP> in the query\n",
    "def replace_sep(examples):\n",
    "    queries = []\n",
    "    for query in examples[\"queries\"]:\n",
    "        queries.append(query.replace(\"</s>\", \"<SEP>\"))\n",
    "    return {\"queries\":queries, \"chunks\":examples[\"chunks\"], \"ids\":examples[\"ids\"], \"labels\":examples[\"labels\"], \"scores\":examples[\"scores\"]}\n",
    "# chunked_dataset = chunked_dataset.map(replace_sep, batched=True, remove_columns=chunked_dataset.column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "{'chunks': \"search snippets Spin City (TV Series 19962002) - IMDb  Comedy ... With Michael J. Fox, Charlie Sheen, Heather Locklear, Richard Kind. ... about the Winston administration's links to organized crime, Mike decides to take ..... In this sitcom, Deputy Mayor Mike Flaherty (Fox) and his City Hall staff must stop ... (Gaberman/Chaplin)the Mayor's Speech Writer, and Nikki Faber (Britton)...\",\n 'ids': 1509,\n 'queries': 'Find the top 5 Web snippets that answer this <SEP> Mayor Randall Winston,Deputy Mayor Michael Flaherty,Nikki Faber',\n 'labels': 1}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_dataset[2333]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home1/cxy/alpaca-lora/process_data/chunked_tart_data/cache-55caf8bb7d7372fa.arrow\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on each chunk and take the chunk with the highest score as the representation of the document for the question\n",
    "# model to device 5\n",
    "\n",
    "\n",
    "def evaluate_model(examples):\n",
    "    # encode\n",
    "    features = tokenizer(examples[\"queries\"], examples[\"chunks\"],padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    features = {k: v.to(device) for k, v in features.items()}\n",
    "    with torch.no_grad():\n",
    "        scores = model(**features).logits\n",
    "        normalized_scores = [float(score[1]) for score in F.softmax(scores, dim=1)]\n",
    "    #save the score of each chunk\n",
    "    return {\"scores\":normalized_scores, \"ids\":examples[\"ids\"], \"labels\":examples[\"labels\"], \"queries\":examples[\"queries\"], \"chunks\":examples[\"chunks\"]}\n",
    "\n",
    "# test_dataset = chunked_dataset.select(range(3600))\n",
    "# test_dataset_scored = test_dataset.map(evaluate_model, batched=True, remove_columns=test_dataset.column_names, batch_size=1280)\n",
    "chunked_dataset_scored = chunked_dataset.map(evaluate_model, batched=True, remove_columns=chunked_dataset.column_names, batch_size=1280)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# save chunked_dataset_scored\n",
    "chunked_dataset_scored_save_path ='/home1/cxy/alpaca-lora/process_data/chunked_tart_data_scored'\n",
    "chunked_dataset_scored.save_to_disk(chunked_dataset_scored_save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "chunked_dataset_scored = datasets.load_from_disk(chunked_dataset_scored_save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/4712048 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47ad1847779a4f7eaee4993adeb8de41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunked_dataset_scored = chunked_dataset_scored.map(replace_sep, batched=True, remove_columns=chunked_dataset_scored.column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "{'chunks': 'To determine if the standard compliance check protocol is a valid measure of the experience of underage smokers when purchasing tobacco in unfamiliar communities. 160 tobacco outlets in eight Massachusetts communities where underage tobacco sales laws are vigorously enforced. Completed purchase rates were compared between underage smokers who behaved normally and inexperienced non-smoking youths who were not allowed to lie or present proof of age (ID). The \"smoker protocol\" increased the likelihood of a sale nearly sixfold over that for the non-smokers (odds ratio (OR) 5.7, 95% confidence interval (CI) 1.5 to 22). When the youths presented an ID with an underage birth date, the odds of a completed sale increased dramatically (OR 27, 95% CI 3.4 to 212). Clerks judged to be under 21 years of age were seven times more likely to make an illegal sale (OR 7.6, 95% CI 2.4 to 24.0). Commonly used compliance check protocols are too artificial to reflect accurately the experience of underage smokers. The validity of compliance checks might be improved by having youths present ID, and by employing either tobacco users, or non-tobacco users who are sufficiently experienced to mimic the self confidence exhibited by tobacco users in this situation. Consideration should be given to prohibiting the sale of tobacco by individuals under 21 years of age.',\n 'ids': 1,\n 'queries': \"Help me to find a highly related pubmed paper to answer this question <SEP> Are the tobacco industry's claims about the size of the illicit cigarette market credible?\",\n 'labels': 0,\n 'scores': 0.038613539189100266}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_dataset_scored[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/4712048 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4442cbaba984dc186e2f214ca7064a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['chunks', 'ids', 'queries', 'labels', 'scores'],\n    num_rows: 4712048\n})"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each ids get the chunk with the highest score\n",
    "from collections import defaultdict\n",
    "# for each id, the initial score is 0\n",
    "id_score = defaultdict(float)\n",
    "# save the idx of the chunk with the highest score\n",
    "max_score_idx = defaultdict(int)\n",
    "def get_max_score(example, idx):\n",
    "    if example[\"scores\"] > id_score[example[\"ids\"]]:\n",
    "        id_score[example[\"ids\"]] = example[\"scores\"]\n",
    "        max_score_idx[example[\"ids\"]] = idx\n",
    "    return example\n",
    "chunked_dataset_scored.map(get_max_score, with_indices=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2990000\n"
     ]
    }
   ],
   "source": [
    "# get the chunk with the highest score\n",
    "idx_list = []\n",
    "for idx in max_score_idx.values():\n",
    "    idx_list.append(idx)\n",
    "print(len(idx_list))\n",
    "chunked_dataset_scored_high = chunked_dataset_scored.select(idx_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "{'chunks': 'assistant to tell her what the problem was. She just didn\\'t know how to fix it. She\\'d been diagnosed with post-traumatic stress disorder, traced to a period of sexual abuse as a child and a life-threatening car accident. She\\'d been hospitalized four times for the debilitating symptoms, and stress had caused bleeding ulcers bad enough to send her to the emergency room twice more. Along the way, she tried cognitive behavioral therapy, hypnosis and acupuncture. She tried an established therapy called eye movement desensitization and reprocessing, where a therapist used physical stimuli -- light tapping and guided eye movements -- to try to retrain her brain. It made her eyeballs feel like they would burst out of her head. She tried gestalt therapy, screaming out her rage. \"Nothing worked,\" she says. \"I got to the point where I just said, \\'I\\'m handicapped. I\\'m just going to have to live my life like this.\\' It was pretty horrible.\" A formal plan . More than 7 million Americans suffer from PTSD, and by most estimates, only half of them -- at best -- are ever cured. A decade ago, the widely acknowledged need for better treatments opened the door to Mithoefer and his unconventional approach. By the time he took Hope\\'s call in February 2005, the soft-spoken, ponytailed Mithoefer had managed to convince the Drug Enforcement Administration to green-light a study of Ecstasy as an adjunct to psychotherapy. Of course, he wasn\\'t calling it Ecstasy. Neither were the scientists from the Food and Drug Administration and certainly not',\n 'ids': 2897,\n 'queries': 'Retrieve a news article that is summarized as following <SEP> Rachel Hope suffered from post-traumatic stress disorder for years .\\nIn 2005, she investigated an experimental new treatment: Ecstasy .\\nDr. Michael Mithoefer convinced the DEA to green-light a study of the treatment .\\nMore than 7 million Americans suffer from PTSD .',\n 'labels': 1,\n 'scores': 0.9456804990768433}"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_dataset_scored_high[2897]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "Saving the dataset (0/6 shards):   0%|          | 0/2990000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b75ee4e5586d43cfa84329ff2cc064ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save chunked_dataset_scored_high\n",
    "chunked_dataset_scored_high_save_path ='/home1/cxy/alpaca-lora/process_data/chunked_tart_data_scored_high'\n",
    "chunked_dataset_scored_high.save_to_disk(chunked_dataset_scored_high_save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# process dev data\n",
    "dev_path = '/home1/cxy/tart/ranker_train_full_contriever_minilm_denoised_w_instruction_unfollowing_10k_added_oqa_fixed/st_dev_ranker_input.json'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/cxy/anaconda3/envs/alpaca-lora/lib/python3.10/site-packages/datasets/load.py:1748: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=no_checks' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home1/cxy/.cache/huggingface/datasets/json/default-fa4895e1595f847b/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa06abeb050a43a5bb2fd4530a0387d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "408fa267ce374c28a384ce9797f86bd4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db1520cadf7940efb530c5a8a68262b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home1/cxy/.cache/huggingface/datasets/json/default-fa4895e1595f847b/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88cba16b2f434218a762f976f2eeaf78"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "dev_data = datasets.load_dataset('json', data_files=dev_path, ignore_verifications=True)['train']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'You have to match this long sentence to a shorter compressed one </s> Andy Murray got back to winning ways by defeating Jesse Huta Galung of the Netherlands 6-4 6-4 in the first round of the ATP Marseille Open on Tuesday.',\n 'document': 'short sentence Murray back to winning',\n 'label': 1}"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 13127.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_doc_len = count_max_doc_token_length(dev_data)\n",
    "print(max_doc_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c53b9558b6b848969619f9a2d5718175"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'query': 'You have to match this long sentence to a shorter compressed one </s> Andy Murray got back to winning ways by defeating Jesse Huta Galung of the Netherlands 6-4 6-4 in the first round of the ATP Marseille Open on Tuesday.',\n 'document': 'short sentence Murray back to winning',\n 'label': 1,\n 'id': 1}"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add id\n",
    "dev_data = dev_data.map(add_ids, with_indices=True)\n",
    "dev_data[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/15559 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48184df9e7d44486bed85774362b4cfe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'chunks': 'short sentence Murray back to winning',\n 'ids': 1,\n 'queries': 'You have to match this long sentence to a shorter compressed one <SEP> Andy Murray got back to winning ways by defeating Jesse Huta Galung of the Netherlands 6-4 6-4 in the first round of the ATP Marseille Open on Tuesday.',\n 'labels': 1,\n 'scores': 0.23419642448425293}"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chunk the data\n",
    "chunked_dev_data = dev_data.map(chunk_examples, batched=True, remove_columns=dev_data.column_names, batch_size=1280)\n",
    "# score the chunks\n",
    "chunked_dev_data_scored = chunked_dev_data.map(evaluate_model, batched=True, remove_columns=chunked_dev_data.column_names, batch_size=1280)\n",
    "# replace </s> with <SEP> in the query\n",
    "chunked_dev_data_scored = chunked_dev_data_scored.map(replace_sep, batched=True, remove_columns=chunked_dev_data_scored.column_names)\n",
    "# get the chunk with the highest score\n",
    "id_score = defaultdict(float)\n",
    "max_score_idx = defaultdict(int)\n",
    "chunked_dev_data_scored.map(get_max_score, with_indices=True)\n",
    "idx_list = []\n",
    "for idx in max_score_idx.values():\n",
    "    idx_list.append(idx)\n",
    "print(len(idx_list))\n",
    "chunked_dev_data_scored_high = chunked_dev_data_scored.select(idx_list)\n",
    "chunked_dev_data_scored_high[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['chunks', 'ids', 'queries', 'labels'],\n    num_rows: 15559\n})"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_dev_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['query', 'document', 'label', 'id'],\n    num_rows: 10000\n})"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['chunks', 'ids', 'queries', 'labels', 'scores'],\n    num_rows: 15559\n})"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_dev_data_scored"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve an paragraph-length answer to this question. <SEP> How accurate is a colorized photograph, and how does it work?\n",
      "You have to match this long sentence to a shorter compressed one <SEP> Andy Murray got back to winning ways by defeating Jesse Huta Galung of the Netherlands 6-4 6-4 in the first round of the ATP Marseille Open on Tuesday.\n",
      "I want to know the answer to the question. Can you find good evidence on the web? <SEP> is sundries group\n",
      "Find the how-to article to achieve the following goal from WikihHow, a website collecting how-to articles. <SEP> Build Bleachers\n",
      "Find a question that is duplicated with the following question asked in Quora, a community QA forum <SEP> What is the question that you've always wanted to ask the world?\n",
      "Find an evidence from medical text book to answer this medical exam question <SEP> Ketone body formation takes place in:\n",
      "WikiHow is an wiki-style community and database of how-to guides. Suggest the best article for the following <SEP> Prevent Renal Artery Stenosis\n",
      "Find a detailed paragraph from WikiHow that explains how-to to achieve <SEP> Pack Lightly for a Family Vacation\n",
      "Find a question that is paraphrased of this <SEP> What is the highest point in France?\n",
      "Retrieve an paragraph-length answer to this question. <SEP> Why do we only look for life on planets with conditions similar to ours?\n",
      "Retrieve a paragraph from Wikipedia that answer this question <SEP> who sang little pig little pig let me in\n",
      "Retrieve a sentence that talks about the same as the following in a simple or shorter English <SEP> The album title is a parody of the The Beatles ' album `` Sgt. Pepper 's Lonely Hearts Club Band '' .\n",
      "Find a question title that is similar to the following question title asked in StackExchange, a community QA forum <SEP> Searching for a string in a column in pandas not working\n",
      "This question asks about the details written in a Wikipedia paragraph. Select the paragraph the question is about <SEP> What houses does the Nigerian legislature have?\n",
      "Find the how-to article to achieve the following goal from WikihHow, a website collecting how-to articles. <SEP> Change Brake Lines\n",
      "Retrieve a good dialogue response that answers this question <SEP> Are there any other movements Rahul Bose was involved in besides assisting in the relief efforts in the boxing day tsunami ?\n",
      "WikiHow is an wiki-style community and database of how-to guides. Suggest the best article for the following <SEP> Recover Lost Luggage\n",
      "Find a related medical paper to answer the following question <SEP> Could leprosy reaction episodes be exacerbated by oral infections?\n",
      "Find a related medical paper to answer the following question <SEP> Could leprosy reaction episodes be exacerbated by oral infections?\n",
      "Find a related medical paper to answer the following question <SEP> Could leprosy reaction episodes be exacerbated by oral infections?\n",
      "I want to know the answer to the question. Can you find good evidence on the web? <SEP> how to use blue symbols on hp laptop keyboard\n",
      "You need to find Wikipedia paragraphs that support or refute this sentence <SEP> Luke Evans has portrayed Dracula.\n",
      "Retrieve a news article that is summarized as following <SEP> The friend is charged with removing a backpack and computer from dorm room .\n",
      "It was the room of accused Boston bomber Dzhokhar Tsarnaev .\n",
      "Dias Kadyrbayev is charged with obstructing justice and conspiracy .\n",
      "He is expected to plead guilty to those federal charges on Thursday .\n",
      "Retrieve a news article that is summarized as following <SEP> The friend is charged with removing a backpack and computer from dorm room .\n",
      "It was the room of accused Boston bomber Dzhokhar Tsarnaev .\n",
      "Dias Kadyrbayev is charged with obstructing justice and conspiracy .\n",
      "He is expected to plead guilty to those federal charges on Thursday .\n",
      "Retrieve a news article that is summarized as following <SEP> The friend is charged with removing a backpack and computer from dorm room .\n",
      "It was the room of accused Boston bomber Dzhokhar Tsarnaev .\n",
      "Dias Kadyrbayev is charged with obstructing justice and conspiracy .\n",
      "He is expected to plead guilty to those federal charges on Thursday .\n",
      "Retrieve a news article that is summarized as following <SEP> The friend is charged with removing a backpack and computer from dorm room .\n",
      "It was the room of accused Boston bomber Dzhokhar Tsarnaev .\n",
      "Dias Kadyrbayev is charged with obstructing justice and conspiracy .\n",
      "He is expected to plead guilty to those federal charges on Thursday .\n",
      "WikiHow is an wiki-style community and database of how-to guides. Suggest the best article for the following <SEP> Know the Difference Between Soft Law and Hard Law (International Law)\n",
      "Retrieve a question that is duplicated with the following <SEP> If you ate a flea what will happen?\n",
      "Find a question that is duplicated with the following question asked in Quora, a community QA forum <SEP> Why is Manaphy anxious in Pokémon Ranger and the Temple of the Sea?\n",
      "I want to find a question similar to this question already asked in Quora. Retrieve a question body that is similar to <SEP> Which dating sites should one consider using, of which it really works?\n",
      "I want to know if this sentence is a fact or not. Can you find relented Wikipedia passages for me? <SEP> Danny DeVito is not a director.\n",
      "You need to find Wikipedia paragraphs that support or refute this sentence <SEP> Liana Liberato starred in a Belgian film directed by Michael Landon Jr.\n",
      "Find a related medical paper to answer the following question <SEP> Does prenatal diagnosis influence the morbidity associated with left in situ nonfunctioning or poorly functioning renal moiety after endoscopic puncture of ureterocele?\n",
      "Find a related medical paper to answer the following question <SEP> Does prenatal diagnosis influence the morbidity associated with left in situ nonfunctioning or poorly functioning renal moiety after endoscopic puncture of ureterocele?\n",
      "Find a related medical paper to answer the following question <SEP> Does prenatal diagnosis influence the morbidity associated with left in situ nonfunctioning or poorly functioning renal moiety after endoscopic puncture of ureterocele?\n",
      "I want to know the answer to the question. Can you find good evidence on the web? <SEP> how do ncaa baseball regionals work\n",
      "I want to find a question similar to this question already asked in Quora. Retrieve a question body that is similar to <SEP> Will there be another Percy Jackson movie?\n",
      "Retrieve passages from Wikipedia to answer the following question <SEP> Which Labour Party shadow cabinet member is married to trade union activist Jack Dromey?\n",
      "Find a question that is duplicated with the following question asked in Quora, a community QA forum <SEP> What age do you think is too late to start medical school, based off of your own experiences?\n",
      "Find an evidence paragraph on the web with evidence and answer for this <SEP> mutual omaha insurance customer service phone number\n",
      "Find a image caption describing the same image as <SEP> Riderless horses walking along the edge of a country highway.\n",
      "Retrieve a web paragraph that answers the following <SEP> what is a binary acid\n",
      "I want to know the answer to the question. Can you find good evidence on the web? <SEP> jesse hughes net worth\n",
      "Find an sentence from Simple Wikipedia that corresponds to the following Wikipedia sentence <SEP> Further expansions of the company 's publishing business include the purchase of how-to publisher Sterling Publishing in 2003 .\n",
      "Retrieve a news article that is summarized as following <SEP> Nicole Kidman's father has  died as a result of a suspected heart attack .\n",
      "It is believed he collapsed inside the restaurant of a Singapore hotel after a morning exercise .\n",
      "Police confirmed Dr Kidman died at Singapore hospital about 10am .\n",
      "Long time friend of the family Wendy Day said he suffered a fall after  eating breakfast .\n",
      "He was visiting Nicole's younger sister, Antonia, who lives there .\n",
      "Dr Kidman was a clinical psychologist at Royal North Shore Hospital .\n",
      "He was also Director of the Health Psychology Unit at University of Technology in Sydney .\n",
      "UTS Vice-Chancellor Professor Attila Brungs said he 'contributed greatly'\n",
      "He was awarded an Order of Australia in 2005 for his contributions to clinical psychology .\n",
      "The Kidman family was last seen together in January before Nicole, Keith Urban and Dr Kidman stepped out onto the red carpet in Sydney in June .\n",
      "Keith Urban has cancelled shows to be with his wife .\n",
      "Retrieve a news article that is summarized as following <SEP> Nicole Kidman's father has  died as a result of a suspected heart attack .\n",
      "It is believed he collapsed inside the restaurant of a Singapore hotel after a morning exercise .\n",
      "Police confirmed Dr Kidman died at Singapore hospital about 10am .\n",
      "Long time friend of the family Wendy Day said he suffered a fall after  eating breakfast .\n",
      "He was visiting Nicole's younger sister, Antonia, who lives there .\n",
      "Dr Kidman was a clinical psychologist at Royal North Shore Hospital .\n",
      "He was also Director of the Health Psychology Unit at University of Technology in Sydney .\n",
      "UTS Vice-Chancellor Professor Attila Brungs said he 'contributed greatly'\n",
      "He was awarded an Order of Australia in 2005 for his contributions to clinical psychology .\n",
      "The Kidman family was last seen together in January before Nicole, Keith Urban and Dr Kidman stepped out onto the red carpet in Sydney in June .\n",
      "Keith Urban has cancelled shows to be with his wife .\n",
      "Retrieve a news article that is summarized as following <SEP> Nicole Kidman's father has  died as a result of a suspected heart attack .\n",
      "It is believed he collapsed inside the restaurant of a Singapore hotel after a morning exercise .\n",
      "Police confirmed Dr Kidman died at Singapore hospital about 10am .\n",
      "Long time friend of the family Wendy Day said he suffered a fall after  eating breakfast .\n",
      "He was visiting Nicole's younger sister, Antonia, who lives there .\n",
      "Dr Kidman was a clinical psychologist at Royal North Shore Hospital .\n",
      "He was also Director of the Health Psychology Unit at University of Technology in Sydney .\n",
      "UTS Vice-Chancellor Professor Attila Brungs said he 'contributed greatly'\n",
      "He was awarded an Order of Australia in 2005 for his contributions to clinical psychology .\n",
      "The Kidman family was last seen together in January before Nicole, Keith Urban and Dr Kidman stepped out onto the red carpet in Sydney in June .\n",
      "Keith Urban has cancelled shows to be with his wife .\n",
      "Retrieve a news article that is summarized as following <SEP> Nicole Kidman's father has  died as a result of a suspected heart attack .\n",
      "It is believed he collapsed inside the restaurant of a Singapore hotel after a morning exercise .\n",
      "Police confirmed Dr Kidman died at Singapore hospital about 10am .\n",
      "Long time friend of the family Wendy Day said he suffered a fall after  eating breakfast .\n",
      "He was visiting Nicole's younger sister, Antonia, who lives there .\n",
      "Dr Kidman was a clinical psychologist at Royal North Shore Hospital .\n",
      "He was also Director of the Health Psychology Unit at University of Technology in Sydney .\n",
      "UTS Vice-Chancellor Professor Attila Brungs said he 'contributed greatly'\n",
      "He was awarded an Order of Australia in 2005 for his contributions to clinical psychology .\n",
      "The Kidman family was last seen together in January before Nicole, Keith Urban and Dr Kidman stepped out onto the red carpet in Sydney in June .\n",
      "Keith Urban has cancelled shows to be with his wife .\n",
      "Retrieve a news article that is summarized as following <SEP> Nicole Kidman's father has  died as a result of a suspected heart attack .\n",
      "It is believed he collapsed inside the restaurant of a Singapore hotel after a morning exercise .\n",
      "Police confirmed Dr Kidman died at Singapore hospital about 10am .\n",
      "Long time friend of the family Wendy Day said he suffered a fall after  eating breakfast .\n",
      "He was visiting Nicole's younger sister, Antonia, who lives there .\n",
      "Dr Kidman was a clinical psychologist at Royal North Shore Hospital .\n",
      "He was also Director of the Health Psychology Unit at University of Technology in Sydney .\n",
      "UTS Vice-Chancellor Professor Attila Brungs said he 'contributed greatly'\n",
      "He was awarded an Order of Australia in 2005 for his contributions to clinical psychology .\n",
      "The Kidman family was last seen together in January before Nicole, Keith Urban and Dr Kidman stepped out onto the red carpet in Sydney in June .\n",
      "Keith Urban has cancelled shows to be with his wife .\n",
      "Retrieve a news article that is summarized as following <SEP> Nicole Kidman's father has  died as a result of a suspected heart attack .\n",
      "It is believed he collapsed inside the restaurant of a Singapore hotel after a morning exercise .\n",
      "Police confirmed Dr Kidman died at Singapore hospital about 10am .\n",
      "Long time friend of the family Wendy Day said he suffered a fall after  eating breakfast .\n",
      "He was visiting Nicole's younger sister, Antonia, who lives there .\n",
      "Dr Kidman was a clinical psychologist at Royal North Shore Hospital .\n",
      "He was also Director of the Health Psychology Unit at University of Technology in Sydney .\n",
      "UTS Vice-Chancellor Professor Attila Brungs said he 'contributed greatly'\n",
      "He was awarded an Order of Australia in 2005 for his contributions to clinical psychology .\n",
      "The Kidman family was last seen together in January before Nicole, Keith Urban and Dr Kidman stepped out onto the red carpet in Sydney in June .\n",
      "Keith Urban has cancelled shows to be with his wife .\n",
      "Find a question that is paraphrased of this <SEP> 2000 s10 rim bolt pattern?\n",
      "Find a question paragraph that is duplicated with the following question paragraph at StackExchange. <SEP> why toLowerCase is not working in my code? In my case i am trying to filter the result from firebase data based on the inputs given by the user. Since firebase is case sensitive iam trying to convert the String Course  into lowercase at both ends inorder to overcome the case sensitivity in the following code.      mUserDatabase.addValueEventListener(new ValueEventListener() {         @Override         public void onDataChange(DataSnapshot dataSnapshot) {              for (DataSnapshot data:dataSnapshot.getChildren()){                   CollegeFilterDetails models=data.getValue(CollegeFilterDetails.class);                        if(   (ClgType.equals(\"\") || ClgType.equals(models.getType()))                         &amp;&amp;(University.equals(\"\")||University.equals(models.getUniversity()))                         &amp;&amp;(Course.equals(\"\")||Course.equals(models.getCourse().toLowerCase()))                         &amp;&amp;(State.equals(\"\")||State.equals(models.getState()))                         &amp;&amp;(District.equals(\"\") || District.equals(models.getDistrict()))                          ){                       filteredList.add(new ObjectModel(models.getName(),models.getPlace(),models.getImage()));                     flag++;                  }             }             Toast.makeText(getBaseContext(), \"found \"+flag+\" results\", Toast.LENGTH_SHORT).show();             adapter.notifyDataSetChanged();          }          @Override         public void onCancelled(DatabaseError databaseError) {          }     });   This throws the error java.lang.NullPointerException: Attempt to invoke virtual method 'java.lang.String java.lang.String.toLowerCase()' on a null object reference when i add toLowerCase() to the Course.\n",
      "Retrieve an sentence-long news summary for this header <SEP> Sequestra classe, scuola si difende\n",
      "Check if a Quora question is duplicated with this question <SEP> What is the best LIC policy?\n",
      "Find an evidence paragraph on the web with evidence and answer for this <SEP> what was food like in the revolutionary war\n",
      "You have to find answer to this question.  <SEP> How a breathalyzer can tell you exactly how intoxicated someone is just by their breath?\n",
      "You have to find answer to this question.  <SEP> If Earth has the conditions for life to be created, why did it only happen once?\n",
      "Can you find an image caption talking about the same image as <SEP> A conveyor belt full of donuts making their way through the front.\n",
      "Find an Wikipedia paragraph that is related to the current conversation topic to generate a meaningful response. <SEP> Ever heard of polydactyly or polydactylism? It's such a cool condition!\n",
      "I have never of that.  Tell me about it.\n",
      "Well polydactyly is basically having supernumerary fingers or toes.\n",
      "What does supernumerary mean?\n",
      "Supernumerary means to have too many, like a person having 6 toes or fingers.\n",
      "Oh wow, how common of a condition is that?\n",
      "Well it isn't too common, but it is even less common on both hands or feet in humans or animals.\n",
      "Do the extra appendages tend to look normal?\n",
      "I want to know the answer to the question. Can you find good evidence on the web? <SEP> what is the salary of a zookeeper\n",
      "Given a news article headline published at npr.org, find a corresponding summary of the news <SEP> Films So Nice, They Named Them Twice?\n",
      "Given a news article headline published at npr.org, find a corresponding summary of the news <SEP> Films So Nice, They Named Them Twice?\n",
      "Given a news article headline published at npr.org, find a corresponding summary of the news <SEP> Films So Nice, They Named Them Twice?\n",
      "Given a news article headline published at npr.org, find a corresponding summary of the news <SEP> Films So Nice, They Named Them Twice?\n",
      "Given a news article headline published at npr.org, find a corresponding summary of the news <SEP> Films So Nice, They Named Them Twice?\n",
      "Given a news article headline published at npr.org, find a corresponding summary of the news <SEP> Films So Nice, They Named Them Twice?\n",
      "Given a news article headline published at npr.org, find a corresponding summary of the news <SEP> Films So Nice, They Named Them Twice?\n",
      "I want to know the answer to the question. Can you find good evidence on the web? <SEP> raising the minimum wage brings many benefits\n",
      "I want to know if this sentence is a fact or not. Can you find relented Wikipedia passages for me? <SEP> 21 Jump Street came out on March 16, 2012.\n",
      "Find an evidence paragraph on the web with evidence and answer for this <SEP> how long can fleas live in the carpet?\n",
      "Retrieve a Wikipedia paragraph that provides useful evidence or answer for this question <SEP> Which of the two bands Kill Hannah and Warpaint is from California?\n",
      "Retrieve a paragraph from Wikipedia to help a conversational AI to generate a knowledge-grounded dialogue <SEP> Hi buddy  I love trumpet\n",
      "Me too! I'm a big fan of instruments that belong to the brass family, jazz is my jam.\n",
      "Trumpet like instrument have  historically been used as signaling devices in battle or hunting\n",
      "That is true, but some time in between the 14th and 15th century it became better known as a musical instrument!\n",
      "Yes during 15th century hey have primarily been constructed of  brass tubing. Its a good instrument\n",
      "I like it because it has a very high register, much higher than that of its brass counterparts. High notes are sweet!\n",
      "Ok good. Its mainly played in the wedding functions and parties. In happy occasions\n",
      "Find an evidence paragraph on the web with evidence and answer for this <SEP> phenylethylamine hcl side effects\n",
      "Retrieve a compressed version of the following sentence written by human annotators <SEP> BARCELONA, Spain -Barcelona has sold reserve-team striker Rochina to Premier League club Blackburn on the final day of a European transfer window that contains few big-money transfers in Spain.\n",
      "You have to match this question to top five web search snippets. <SEP> A Russian immigrant mouse named Fievel Mousekewitz is the hero of this 1986 animated film\n",
      "Retrieve passages from Wikipedia to answer the following question <SEP> In October 1992, which baseball team became the first non-USA team to win the World Series?\n",
      "I want to know the answer to the question. Can you find good evidence on the web? <SEP> can silver united member get economy plus\n",
      "A simplified sentence from SimpleWikipedia of this Wikipedia sentence  <SEP> The first two counties , Des Moines County and Dubuque County , were created in 1834 when Iowa was still part of the Michigan Territory .\n",
      "I want to know the answer to the question. Can you find good evidence on the web? <SEP> logistic growth definition apex\n",
      "Find a question that is duplicated with the following question asked in Quora, a community QA forum <SEP> Can I be arrested for Downloading TV series from torrent in India?\n",
      "Find an answer to this question for me. <SEP> How did native Americans deal with such harsh winter temperatures (Especially polar vortexes)?\n",
      "Find a Wikipedia paragraph that answer the question <SEP> What was Pelayos' plan?\n",
      "Find a Wikipedia paragraph that answer the question <SEP> What was Pelayos' plan?\n",
      "Retrieve passages from Wikipedia to answer the following question <SEP> Which musical with book and lyrics by James Rado and Gerome Ragni and music by Galt MacDermot opened in October 1967 at Joseph Papp's Public Theater?\n",
      "Retrieve a image caption sentence that is written for the same image as the following caption <SEP> A red train traveling down train tracks in a city.\n",
      "The following sentences are the summaries of an news article. Find the source news article. <SEP> Nanny Yoselyn Ortega is alleged to have killed Lucia and Leo Krim  last year .\n",
      "The two children were found with horrific injuries by mother Marina Krim .\n",
      "Ortega's sister babysits girl at same school as Krims' surviving child Nessie .\n",
      "Celia Ortega helped Yoselyn get the job with the Krim family in New York .\n",
      "New York Post claims it has seen email from Marina describing her anguish .\n",
      "The following sentences are the summaries of an news article. Find the source news article. <SEP> Nanny Yoselyn Ortega is alleged to have killed Lucia and Leo Krim  last year .\n",
      "The two children were found with horrific injuries by mother Marina Krim .\n",
      "Ortega's sister babysits girl at same school as Krims' surviving child Nessie .\n",
      "Celia Ortega helped Yoselyn get the job with the Krim family in New York .\n",
      "New York Post claims it has seen email from Marina describing her anguish .\n",
      "The following sentences are the summaries of an news article. Find the source news article. <SEP> Nanny Yoselyn Ortega is alleged to have killed Lucia and Leo Krim  last year .\n",
      "The two children were found with horrific injuries by mother Marina Krim .\n",
      "Ortega's sister babysits girl at same school as Krims' surviving child Nessie .\n",
      "Celia Ortega helped Yoselyn get the job with the Krim family in New York .\n",
      "New York Post claims it has seen email from Marina describing her anguish .\n",
      "The following sentences are the summaries of an news article. Find the source news article. <SEP> Nanny Yoselyn Ortega is alleged to have killed Lucia and Leo Krim  last year .\n",
      "The two children were found with horrific injuries by mother Marina Krim .\n",
      "Ortega's sister babysits girl at same school as Krims' surviving child Nessie .\n",
      "Celia Ortega helped Yoselyn get the job with the Krim family in New York .\n",
      "New York Post claims it has seen email from Marina describing her anguish .\n",
      "The following sentences are the summaries of an news article. Find the source news article. <SEP> Nanny Yoselyn Ortega is alleged to have killed Lucia and Leo Krim  last year .\n",
      "The two children were found with horrific injuries by mother Marina Krim .\n",
      "Ortega's sister babysits girl at same school as Krims' surviving child Nessie .\n",
      "Celia Ortega helped Yoselyn get the job with the Krim family in New York .\n",
      "New York Post claims it has seen email from Marina describing her anguish .\n",
      "The following sentences are the summaries of an news article. Find the source news article. <SEP> Nanny Yoselyn Ortega is alleged to have killed Lucia and Leo Krim  last year .\n",
      "The two children were found with horrific injuries by mother Marina Krim .\n",
      "Ortega's sister babysits girl at same school as Krims' surviving child Nessie .\n",
      "Celia Ortega helped Yoselyn get the job with the Krim family in New York .\n",
      "New York Post claims it has seen email from Marina describing her anguish .\n",
      "The following sentences are the summaries of an news article. Find the source news article. <SEP> Nanny Yoselyn Ortega is alleged to have killed Lucia and Leo Krim  last year .\n",
      "The two children were found with horrific injuries by mother Marina Krim .\n",
      "Ortega's sister babysits girl at same school as Krims' surviving child Nessie .\n",
      "Celia Ortega helped Yoselyn get the job with the Krim family in New York .\n",
      "New York Post claims it has seen email from Marina describing her anguish .\n",
      "The following sentences are the summaries of an news article. Find the source news article. <SEP> Nanny Yoselyn Ortega is alleged to have killed Lucia and Leo Krim  last year .\n",
      "The two children were found with horrific injuries by mother Marina Krim .\n",
      "Ortega's sister babysits girl at same school as Krims' surviving child Nessie .\n",
      "Celia Ortega helped Yoselyn get the job with the Krim family in New York .\n",
      "New York Post claims it has seen email from Marina describing her anguish .\n",
      "Check if a Quora question is duplicated with this question <SEP> How does the HP OfficeJet 4620 Airprint compare to the HP Color LaserJet Pro M452dw?\n",
      "I want to find a question similar to this question already asked in Quora. Retrieve a question body that is similar to <SEP> What ethnicity were the Moors of Sicily?\n",
      "Retrieve an paragraph-length answer to this question. <SEP> How do cellphone and bluetooth signals travel through walls??\n",
      "Retrieve an paragraph-length answer to this question. <SEP> How does Shazam actually work?\n",
      "Find a question paragraph that is duplicated with the following question paragraph at StackExchange. <SEP> How do I install the Nvidia driver for a GeForce GT 630 I recently installed 14.04.  But now I need a new driver for my nVidia GeForce GT 630. The former driver was rejected as not compatible with the 64-bit. I found that other driver and when I wanted to install it in the terminal with sh I was called first to stop the x-server. It cannot be installed with running x-server.  So how do I install them?\n",
      "Retrieve a web paragraph that answers the following <SEP> what is core pulmonale\n",
      "Retrieve a web paragraph that answers the following <SEP> what is federalism pdf\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(chunked_dev_data_scored[i][\"queries\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf071460752c4818800cdb80ac48ac5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save chunked_dev_data_scored_high\n",
    "chunked_dev_data_scored_high_save_path ='/home1/cxy/alpaca-lora/process_data/chunked_dev_data_scored_high'\n",
    "chunked_dev_data_scored_high.save_to_disk(chunked_dev_data_scored_high_save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}